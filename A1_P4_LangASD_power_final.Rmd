---
title: "Assignment 1 - Language Development in ASD - part 4"
author: "Riccardo Fusaroli"
date: "August 10, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Welcome to the fourth exciting part of the Language Development in ASD exercise

In this exercise we will assess how many participants we would need to adequately replicate our findings (ensuring our sample size is adequate, our alpha at 0.05 and our beta at 0.8).

```{r}
# Load libraries
library(lmtest)
library(lme4)
library(lmerTest)
library(ggplot2)
library(dplyr)
library(MuMIn)
library(car)
library(plyr)
library(stringr)
library(tidyverse)
library(Metrics)
library(modelr)
library(caret)
library(cvTools)
library(simr)
library(MASS)
```

### Exercise 1

How much power does your study have (if your model estimates are quite right)?
- [GitHub]Load your dataset, fit your favorite model, assess power for your main effects and interactions of interest.
```{r}
# Set WD and read data
setwd("C:/Users/Karolina/Desktop/AU/Experimental Methods III/Assignments/ExpM3-A2")
d_train = read.csv("A1_train.csv")

# Model
m = lmer(CHI_MLU ~ 1 + VISIT + Diagnosis + types_CHI + verbalIQ + (1+VISIT|SUBJ), data=d_train)
summary(m)
# R2
r.squaredGLMM(m)
```

```{r}
# Calculate power for each fixed effect - the smaller the effect size, the more participants we need to get adequate power

powerVisit=powerSim(m,fixed("VISIT"),nsim=50)
powerVisit

powerDiagnosis=powerSim(m,fixed("Diagnosis"),nsim=50)
powerDiagnosis

powerTypesCHI=powerSim(m,fixed("types_CHI"),nsim=50)
powerTypesCHI

powerVerbalIQ=powerSim(m,fixed("verbalIQ"),nsim=50)
powerVerbalIQ
```

- Report the power analysis and comment on what you can (or cannot) use its estimates for.

To calculate the power of our study, 50 simulations were run on each predictor in the model. 
•	For ‘visit’, we get power of 92.00% (with 95% confidence interval ranging from 80.77 to 97.78), and effect size of 0.077.
•	For ‘diagnosis’, we get power of 18.00% (with 95% confidence interval ranging from 8.58 to 31.44).
•	For ‘types of words', we get power of 100.00% (with 95% confidence interval ranging from 92.89 to 100.00), and effect size of 0.0081.
•	For verbal IQ, we get power of 82.00% (with 95% confidence interval ranging from 68.56 to 91.42), and effect size of 0.020.

In order to have adequate power, the percentage should be 80 or higher. In our case, we got adequate power for verbal IQ (82%), visit (92%), and types of words (100%).
Knowing the estimates of the power is useful for assessing whether we should trust the values in the statistical analyses, and whether we need more participants (given the effect size). Thus, the power analysis is a good way to evaluate whether we have sufficiently considered the methods used in our study. 



### Exercise 2

How would you perform a more conservative power analysis?
- Identify and justify a minimum effect size for each of your relevant effects
- [GitHub] take the model from exercise 1 and replace the effects with the minimum effect size that you'd accept.
- [GitHub] assess the power curve by Child.ID, identifying an ideal number of participants to estimate each effect
- OPTIONAL if your power estimates do not reach an acceptable threshold simulate additional participants and repeat the previous analysis
- Report the power analysis and comment on what you can (or cannot) use its estimates for.


```{r}
# Look at the estimates in your model, choose a bit smaller estimates for the min effect size
    # Ususally we would look at effect sizes in previous/pilot studies

# Min effect size
fixef(m)["VISIT"]<-0.06
fixef(m)["DiagnosisTD"]<-0.05
fixef(m)["types_CHI"]<-0.007
fixef(m)["verbalIQ"]<-0.02
```

```{r}
# Power Curve
  # Visit
powerCurveVisit = powerCurve(m,fixed("VISIT"),along="SUBJ",nsim=50)
plot(powerCurveVisit)

  # Diagnosis
powerCurveDiagnosis = powerCurve(m,fixed("DiagnosisTD"),along="SUBJ",nsim=50)
plot(powerCurveDiagnosis)

  # Types_CHI
powerCurveTypesCHI = powerCurve(m,fixed("types_CHI"),along="SUBJ",nsim=50)
plot(powerCurveTypesCHI)

  # VerbalIQ
powerCurveVerbalIQ = powerCurve(m,fixed("verbalIQ"),along="SUBJ",nsim=50)
plot(powerCurveVerbalIQ)
```

The power curve for the fixed effect ‘visit’ shows that, given the specified effect size, we would need 55 participants to get at least 80% of power. For ‘types_CHI’, we would need around 6 participants to achieve power of 80%. Similarly to ‘visit’, we would need 55 participants for ‘verbalIQ’ to get power of 80%. Finally, the power curve for ‘diagnosis’ is flat all the way, so it seems that it does not make sense to do the power analysis at all (it was, after all, the only predictor that did not achieve enough power in the previous exercise).


```{r}
# NOT USING THIS

### Riccardo's clumsy function to simulate new participants
### TO DO points are only notes for myself, so not part of the assignment


createNewData = function(participants,visits,model){
  # participants is the number of subjects
  # visits is the number of visits
  # TO DO: LOOP THROUGH ALL FE ROWS AND AUTOMATICALLY EXTRACT NAMES OF FIXED EFFECTS AND ESTIMATES
  fe <- fixef(model)
  Intercept <- fe[1] #intercept
  bVISIT <- fe[2] #visit
  bDiagnosis <- fe[3] #diagnosis
  bVISITDiagnosis <- fe[4] #visit diagnosis interaction
  # TO DO: INTEGRATE STANDARD ERROR?
  
  # TO DO: LOOP THROUGH ALL VC COMPONENTS AND AUTOMATICALLY EXTRACT NAMES OF EFFECTS AND ESTIMATES
  vc<-VarCorr(model) # variance component
  sigmaSubject <- as.numeric(attr(vc[[1]],"stddev")[1]) # random intercept by subject
  sigmaVISIT <- as.numeric(attr(vc[[1]],"stddev")[2]) # random slope of visit over subject
  sigmaResiduals <- as.numeric(attr(vc,"sc"))
  sigmaCorrelation <- as.numeric(attr(vc[[1]],"correlation")[2])
  
  # Create an empty dataframe
  d=expand.grid(VISIT=1:visits,SUBJ=1:participants)
  # Randomly sample from a binomial (to generate the diagnosis)
  condition <- sample(rep(0:1, participants/2))
  d$Diagnosis<-condition[d$SUBJ]
  d$Diagnosis[is.na(d$Diagnosis)]<-1
  
  ## Define variance covariance matrices:
  Sigma.u<-matrix(c(sigmaSubject^2,
                    sigmaCorrelation*sigmaSubject*sigmaVISIT,
                    sigmaCorrelation*sigmaSubject*sigmaVISIT,
                    sigmaVISIT^2),nrow=2)
  
  ## generate new fake participants (column1=RandomIntercept, column2=RandomSlope)
  u<-mvrnorm(n=participants,
             mu=c(0,0),Sigma=cov(ranef(m)$SUBJ))
  
  ## now generate fake data:
  ### the outcome is extracted from a gaussian with
  ### the solution to the model's equation as mean and
  ### the residual standard deviation as standard deviation 
  d$CHI_MLU <- rnorm(participants*visits,
                     (Intercept+u[,1]) +
                     (bVISIT+u[,2])*d$VISIT + 
                     bDiagnosis*d$Diagnosis ,sigmaResiduals)  
  
  return(d)
}

dsim = createNewData(100,6,m)
```


### Exercise 3

Assume you have only the resources to collect 30 kids (15 with ASD and 15 TDs). Identify the power for each relevant effect and discuss whether it's worth to run the study and why.

```{r}
# Create subset with 30 kids 
d_thirty = subset(d_train, SUBJ<33 & SUBJ!=28 & SUBJ!=31)
       
# Fit model on the subset
m2 = lmer(CHI_MLU ~ 1 + VISIT + Diagnosis + types_CHI + verbalIQ + (1+VISIT|SUBJ), data=d_thirty)
```

```{r}
# Calculate power for each fixed effect - the smaller the effect size, the more participants we need to get adequate power

powerVisit=powerSim(m2,fixed("VISIT"),nsim=50)
powerVisit

powerDiagnosis=powerSim(m2,fixed("Diagnosis"),nsim=50)
powerDiagnosis

powerTypesCHI=powerSim(m2,fixed("types_CHI"),nsim=50)
powerTypesCHI

powerVerbalIQ=powerSim(m2,fixed("verbalIQ"),nsim=50)
powerVerbalIQ
```
To calculate the power of study with 30 kids, 50 simulations were run on each of the predictors:
•	For ‘visit’, we get power of 70.00% (with 95% confidence interval ranging from 55.39 to 82.14), and effect size of 0.082.
•	For ‘diagnosis’, we get power of 6.00% (with 95% confidence interval ranging from 1.25 to 16.55).
•	For ‘types of words’, we get power of 100.00% (with 95% confidence interval ranging from 92.89 to 100.00), and effect size of 0.0082.
•	For verbal IQ, we get power of 74.00% (with 95% confidence interval ranging from 59.66 to 85.37), and effect size of 0.025.

We got adequate power only for the fixed effect ‘types of words' (100%), the other fixed effects have inadequate power. Using fewer kids gives bigger effect sizes, but the study would not be worth running because of the inadequate power. More participants would be desirable. 







